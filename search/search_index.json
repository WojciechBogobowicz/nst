{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neural Style Trainsfer Project","text":""},{"location":"#project-description","title":"Project description","text":"<p>The goal of this project was to create neural style transfer (NST) model, that allows user to create unique and cool graphics without ability to draw ;) by transfering style of one image to another. Details how NST work, can be found in this paper. This project was also inspired by this tensorflow tutorial and Andrew Ng CNN course. </p>"},{"location":"#source-code","title":"Source code","text":"<p>You can find source code to this project on github: https://github.com/WojciechBogobowicz/nst </p>"},{"location":"images/","title":"Images","text":"<p>Images folder contains two types of subfolders:  </p> <ul> <li>Input subfolders - <code>contents</code> and <code>styles</code> folders where content and style images prepared for training are stored.  </li> <li>Output subfolders - are folders prepared for <code>save_vizualizations()</code> function output. This function saves two images. First of them is an image generated by NSTImageTrainer and it is saved in <code>results</code> subfolder. Second is NSTImageTrainer summary schema generated by <code>plot_trainer()</code> function. It is saved in <code>trainers</code> subfolder.  </li> </ul>"},{"location":"installation/","title":"Environment setup","text":"<p><code>Note:</code> Environment setups presented below do not include GPU acceleration initialization. If you want to use GPU, you need to install proper CUDA for your graphic card. Details how to do it, can be found on tensorflow installation guide.</p>"},{"location":"installation/#without-virtual-envs","title":"Without virtual envs","text":"<p>This approach isn't recommended due to potential dependency conflicts that may occur during installation process, but if it will work in your default environment it would be the fastest way.</p> <pre><code>pip install -r ./requirements/raw_requirements.txt\n</code></pre>"},{"location":"installation/#conda-installation","title":"Conda installation","text":"<pre><code>conda env create --file ./requirements/environment.yaml\n</code></pre>"},{"location":"installation/#pip-installation","title":"Pip installation","text":"<p><code>Note:</code> Pip installation require python in version between 3.7-3.10 </p> <p>If you want to create virtual env by your own in different way (for example using ide), then to it and go to step four. 1. Install library for virtual environments if you don't have it yet.</p> <pre><code>pip install virtualenv\n</code></pre> <ol> <li>Create new virtual environment.</li> </ol> <pre><code>virtualenv nst_env\n</code></pre> <ol> <li>Activate it Windows:  </li> </ol> <pre><code>./nst_env/Scripts/Activate.ps1\n</code></pre> <p>Linux:  </p> <pre><code>source ./nst_env/bin/activate\n</code></pre> <ol> <li>Install requirements from file:</li> </ol> <pre><code>pip install -r ./requirements/pip_requirements.txt\n</code></pre>"},{"location":"model/","title":"Model","text":"<p><code>model.py</code> is a core file that contains definition of model used in neural style training. This model is split into two classes. <code>StyleContentExtractor</code> that implements forward pass of the model and <code>NSTImageTrainer</code> that uses it to perform training.   </p>"},{"location":"model/#src.model.NSTImageTrainer","title":"<code>NSTImageTrainer</code>","text":"<p>         Bases: <code>tf.keras.models.Model</code></p> Source code in <code>src/model.py</code> <pre><code>class NSTImageTrainer(tf.keras.models.Model):\n    def __init__(\n        self,\n        style_image: tf.Tensor,\n        content_image: tf.Tensor,\n        style_layers: list[str],\n        content_layers: list[str],\n        content_weight: float = 1e4,\n        style_weight: float = 1e-2,\n        total_variation_weight: int = 30,\n        noise: float = 0,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n\"\"\"NSTImageTrainer is able to generate image based on style and content images.\n        It is based on pretrained vgg model, and Neural Style Transfer technique.\n\n        Args:\n            style_image (tf.Tensor): 3D Tensor represents style image\n            content_image (tf.Tensor): 3D Tensor represents content image\n            style_layers (list[str]): list of all vgg layers names used to calculate style.\n                List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names().\n            content_layers (list[str]): list of all vgg layers names used to calculate content\n                (Usualy one layer is recomended).\n                List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names()\n            content_weight (float, optional): Weight of content loss. Defaults to 1e4.\n            style_weight (float, optional): Weigth of style loss. Defaults to 1e-2.\n            total_variation_weight (int, optional): Weight of total variation,\n                increasing this should help to reduce artefacts in generated image. Defaults to 30.\n            noise (float, optional): Amount of noise added to generated image on the beggining. Defaults to 0.25.\n\n        Tips:\n        In default paramteres role of style is much smaller than content. This aproach is suited for CPU training,\n        because it gives good results faster. On the other hand if you use GPU,\n        then increasing importance of style can give you more interesting images after longer traing.\n\n        If you want to generate image more symilar to content image,\n        you should use one of the ealrier layers as content layer, set small noise (or 0)\n        and use content_weigth bigger than style weight.\n\n        If you want to generate image more symilar to style image,\n        you should use one of the ending layers as content layer, set big noise\n        and use style_weigth bigger than content weight.\n\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.extractor = StyleContentExtractor(style_layers, content_layers)\n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.num_style_layers = len(style_layers)\n        self.num_content_layers = len(content_layers)\n        self.style_targets = self.extractor(style_image)[\"style\"]\n        self.content_targets = self.extractor(content_image)[\"content\"]\n        noise = tf.random.uniform(tf.shape(content_image), -noise, noise)\n        output_image = tf.add(content_image, noise)\n        output_image = tf.clip_by_value(\n            output_image, clip_value_min=0.0, clip_value_max=1.0\n        )\n        self.__output_image = tf.Variable(output_image)\n        self.style_weight = style_weight\n        self.content_weight = content_weight\n        self.total_variation_weight = total_variation_weight\n        self.__style_image = style_image\n        self.__content_image = content_image\n\n    def training_loop(\n        self,\n        steps_per_epoch: int,\n        epochs: int,\n        callbacks: list[Callable[[], None]] = None,\n    ) -&gt; None:\n\"\"\"Train output image. After every epoch callbacks are called.\n\n        Args:\n            epochs (int): Nuber of epoch in training.\n            steps (int): Training steps in every epoch. After each step gradiends are applied to image.\n            callbacks (list[Callable[[],]], optional): List of callable objects,\n            that are called on the end of every trainer training.\n            They cannot take any parameters. If you need to use function with parameters,\n            wrap it like in exapmle below. Defaults to None.\n\n        Callback wrap examples:\n\n        def callback_without_arg():\n            callback(global_arg1, global_arg2)\n\n        or\n\n        class CallbackObj:\n            def __init__(arg1, arg2, callback):\n                self.arg1 = arg1\n                self.arg2 = arg2\n                self.callback = callback\n\n            def __call__(self):\n                self.callback(self.arg1, self.arg2)\n        \"\"\"\n        for epoch_num in range(1, epochs + 1):\n            print(f\"Epoch {epoch_num}/{epochs}:\")\n            for _ in tqdm(range(steps_per_epoch)):\n                self.train_step()\n            if callbacks:\n                for callback in callbacks:\n                    callback()\n\n    @tf.function()\n    @tf.autograph.experimental.do_not_convert\n    def train_step(self) -&gt; None:\n\"\"\"Perform the actual training step.\"\"\"\n        with tf.GradientTape() as tape:\n            outputs = self.extractor(self.__output_image)\n            loss = self.__style_content_loss(outputs)\n            loss += self.__calculate_total_variation()\n\n        grad = tape.gradient(loss, self.__output_image)\n        self.optimizer.apply_gradients([(grad, self.__output_image)])\n        self.__output_image.assign(utils.tf_utils.clip_0_1(self.__output_image))\n\n    def __style_content_loss(self, outputs: dict[str, dict]) -&gt; float:\n\"\"\"Compute the style and content loss based on output from StyleContentExtractor.\n\n        Args:\n            outputs (dict[str, dict]): output from StyleContentExtractor call.\n\n        Returns:\n            float: Total weighted style and content loss.\n        \"\"\"\n        style_outputs = outputs[\"style\"]\n        content_outputs = outputs[\"content\"]\n        style_loss = tf.add_n(\n            [\n                tf.reduce_mean((style_outputs[name] - self.style_targets[name]) ** 2)\n                for name in style_outputs.keys()\n            ]\n        )\n        style_loss *= self.style_weight / self.num_style_layers\n\n        content_loss = tf.add_n(\n            [\n                tf.reduce_mean(\n                    (content_outputs[name] - self.content_targets[name]) ** 2\n                )\n                for name in content_outputs.keys()\n            ]\n        )\n        content_loss *= self.content_weight / self.num_content_layers\n        loss = style_loss + content_loss\n        return loss\n\n    def __calculate_total_variation(self) -&gt; float:\n\"\"\"Calculates the total variation of the output image.\n\n        Returns:\n            [float]: The total, weighted, variation of output image.\n        \"\"\"\n        return self.total_variation_weight * tf.image.total_variation(\n            self.__output_image\n        )\n\n    @property\n    def metrics(self) -&gt; list:\n\"\"\"Return a list of tracked metrics. In this case only loss is tracked,\n            but list is returned because of compatibility with tensorflow.\n\n        Returns:\n            [list[tf.keras.metrics.Mean]]: tf.keras.metrics.Mean object wrapped in the list.\n        \"\"\"\n        return [self.loss_tracker]\n\n    @property\n    def output_image(self) -&gt; PIL.Image:\n\"\"\"Outputs the image representation of generated image tensor.\n\n        Returns:\n            PIL.Image\n        \"\"\"\n        return utils.tf_utils.tensor_to_image(self.__output_image)\n\n    @property\n    def style_image(self) -&gt; PIL.Image:\n\"\"\"Outputs the image representation of style image tensor.\n\n        Returns:\n            PIL.Image\n        \"\"\"\n        return utils.tf_utils.tensor_to_image(self.__style_image)\n\n    @property\n    def content_image(self) -&gt; PIL.Image:\n\"\"\"Outputs the image representation of content image tensor.\n\n        Returns:\n            PIL.Image\n        \"\"\"\n        return utils.tf_utils.tensor_to_image(self.__content_image)\n\n    @staticmethod\n    def model_layers_names() -&gt; list[str]:\n\"\"\"Returns a list of all model layers names.\n\n        Returns:\n            list[str]\n        \"\"\"\n        return [layer.name for layer in StyleContentExtractor.base_model.layers]\n\n    @classmethod\n    def from_layers_selectors(\n        self,\n        style_image: tf.Tensor,\n        content_image: tf.Tensor,\n        style_layers_selector: Callable[[list[str]], list[str]],\n        content_layers_selector: Callable[[list[str]], list[str]],\n        trainer_kw=dict(total_variation_weight=30),\n        style_layers_selector_kw: dict = dict(),\n        content_layers_selector_kw: dict = dict(),\n    ):\n\"\"\"Create a NSTImageTrainer from a style_layers_selector and content_layler_selector functions.\n\n        Args:\n            style_image (tf.Tensor): 3D Tensor represents style image.\n            content_image (tf.Tensor): 3D Tensor represents content image.\n            style_layers_selector (Callable[[list[str]], list[str]]): Function, that takes as arugment\n                list of all convolutional layers names and return subset of them. Returned list will be used\n                as style layers in currently created NSTImageTrainer.\n            content_layers_selector (Callable[[list[str]], list[str]]): Function, that takes as arugment\n                list of all convolutional layers names and return subset of them. Returned list will be used\n                as content layers in currently created NSTImageTrainer.\n            trainer_kw (dict, optional): Keyword arguments to create NSTImageTrainer.\n                Defaults to {\"total_variation_weight\": 30}.\n            style_layers_selector_kw ([dict], optional): Keywords arguments of style_layers_selector.\n                Defaults to dict().\n            content_layers_selector_kw ([dict], optional): Keywords arguments of content_layers_selector.\n                Defaults to dict().\n\n        Returns:\n            NSTImageTrainer\n        \"\"\"\n        layers = NSTImageTrainer.model_layers_names()\n        conv_layers = [name for name in layers if \"conv\" in name]\n\n        style_layers = style_layers_selector(conv_layers, **style_layers_selector_kw)\n        content_layers = content_layers_selector(\n            conv_layers, **content_layers_selector_kw\n        )\n\n        trainer = NSTImageTrainer(\n            style_image, content_image, style_layers, content_layers, **trainer_kw\n        )\n        return trainer\n</code></pre>"},{"location":"model/#src.model.NSTImageTrainer.content_image","title":"<code>content_image: PIL.Image</code>  <code>property</code>","text":"<p>Outputs the image representation of content image tensor.</p> <p>Returns:</p> Type Description <code>PIL.Image</code> <p>PIL.Image</p>"},{"location":"model/#src.model.NSTImageTrainer.metrics","title":"<code>metrics: list</code>  <code>property</code>","text":"<p>Return a list of tracked metrics. In this case only loss is tracked,     but list is returned because of compatibility with tensorflow.</p> <p>Returns:</p> Type Description <code>list</code> <p>[list[tf.keras.metrics.Mean]]: tf.keras.metrics.Mean object wrapped in the list.</p>"},{"location":"model/#src.model.NSTImageTrainer.output_image","title":"<code>output_image: PIL.Image</code>  <code>property</code>","text":"<p>Outputs the image representation of generated image tensor.</p> <p>Returns:</p> Type Description <code>PIL.Image</code> <p>PIL.Image</p>"},{"location":"model/#src.model.NSTImageTrainer.style_image","title":"<code>style_image: PIL.Image</code>  <code>property</code>","text":"<p>Outputs the image representation of style image tensor.</p> <p>Returns:</p> Type Description <code>PIL.Image</code> <p>PIL.Image</p>"},{"location":"model/#src.model.NSTImageTrainer.__calculate_total_variation","title":"<code>__calculate_total_variation()</code>","text":"<p>Calculates the total variation of the output image.</p> <p>Returns:</p> Type Description <code>float</code> <p>[float]: The total, weighted, variation of output image.</p> Source code in <code>src/model.py</code> <pre><code>def __calculate_total_variation(self) -&gt; float:\n\"\"\"Calculates the total variation of the output image.\n\n    Returns:\n        [float]: The total, weighted, variation of output image.\n    \"\"\"\n    return self.total_variation_weight * tf.image.total_variation(\n        self.__output_image\n    )\n</code></pre>"},{"location":"model/#src.model.NSTImageTrainer.__init__","title":"<code>__init__(style_image, content_image, style_layers, content_layers, content_weight=10000.0, style_weight=0.01, total_variation_weight=30, noise=0, *args, **kwargs)</code>","text":"<p>NSTImageTrainer is able to generate image based on style and content images. It is based on pretrained vgg model, and Neural Style Transfer technique.</p> <p>Parameters:</p> Name Type Description Default <code>style_image</code> <code>tf.Tensor</code> <p>3D Tensor represents style image</p> required <code>content_image</code> <code>tf.Tensor</code> <p>3D Tensor represents content image</p> required <code>style_layers</code> <code>list[str]</code> <p>list of all vgg layers names used to calculate style. List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names().</p> required <code>content_layers</code> <code>list[str]</code> <p>list of all vgg layers names used to calculate content (Usualy one layer is recomended). List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names()</p> required <code>content_weight</code> <code>float</code> <p>Weight of content loss. Defaults to 1e4.</p> <code>10000.0</code> <code>style_weight</code> <code>float</code> <p>Weigth of style loss. Defaults to 1e-2.</p> <code>0.01</code> <code>total_variation_weight</code> <code>int</code> <p>Weight of total variation, increasing this should help to reduce artefacts in generated image. Defaults to 30.</p> <code>30</code> <code>noise</code> <code>float</code> <p>Amount of noise added to generated image on the beggining. Defaults to 0.25.</p> <code>0</code> <p>Tips: In default paramteres role of style is much smaller than content. This aproach is suited for CPU training, because it gives good results faster. On the other hand if you use GPU, then increasing importance of style can give you more interesting images after longer traing.</p> <p>If you want to generate image more symilar to content image, you should use one of the ealrier layers as content layer, set small noise (or 0) and use content_weigth bigger than style weight.</p> <p>If you want to generate image more symilar to style image, you should use one of the ending layers as content layer, set big noise and use style_weigth bigger than content weight.</p> Source code in <code>src/model.py</code> <pre><code>def __init__(\n    self,\n    style_image: tf.Tensor,\n    content_image: tf.Tensor,\n    style_layers: list[str],\n    content_layers: list[str],\n    content_weight: float = 1e4,\n    style_weight: float = 1e-2,\n    total_variation_weight: int = 30,\n    noise: float = 0,\n    *args,\n    **kwargs,\n) -&gt; None:\n\"\"\"NSTImageTrainer is able to generate image based on style and content images.\n    It is based on pretrained vgg model, and Neural Style Transfer technique.\n\n    Args:\n        style_image (tf.Tensor): 3D Tensor represents style image\n        content_image (tf.Tensor): 3D Tensor represents content image\n        style_layers (list[str]): list of all vgg layers names used to calculate style.\n            List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names().\n        content_layers (list[str]): list of all vgg layers names used to calculate content\n            (Usualy one layer is recomended).\n            List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names()\n        content_weight (float, optional): Weight of content loss. Defaults to 1e4.\n        style_weight (float, optional): Weigth of style loss. Defaults to 1e-2.\n        total_variation_weight (int, optional): Weight of total variation,\n            increasing this should help to reduce artefacts in generated image. Defaults to 30.\n        noise (float, optional): Amount of noise added to generated image on the beggining. Defaults to 0.25.\n\n    Tips:\n    In default paramteres role of style is much smaller than content. This aproach is suited for CPU training,\n    because it gives good results faster. On the other hand if you use GPU,\n    then increasing importance of style can give you more interesting images after longer traing.\n\n    If you want to generate image more symilar to content image,\n    you should use one of the ealrier layers as content layer, set small noise (or 0)\n    and use content_weigth bigger than style weight.\n\n    If you want to generate image more symilar to style image,\n    you should use one of the ending layers as content layer, set big noise\n    and use style_weigth bigger than content weight.\n\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n    self.extractor = StyleContentExtractor(style_layers, content_layers)\n    self.style_layers = style_layers\n    self.content_layers = content_layers\n    self.num_style_layers = len(style_layers)\n    self.num_content_layers = len(content_layers)\n    self.style_targets = self.extractor(style_image)[\"style\"]\n    self.content_targets = self.extractor(content_image)[\"content\"]\n    noise = tf.random.uniform(tf.shape(content_image), -noise, noise)\n    output_image = tf.add(content_image, noise)\n    output_image = tf.clip_by_value(\n        output_image, clip_value_min=0.0, clip_value_max=1.0\n    )\n    self.__output_image = tf.Variable(output_image)\n    self.style_weight = style_weight\n    self.content_weight = content_weight\n    self.total_variation_weight = total_variation_weight\n    self.__style_image = style_image\n    self.__content_image = content_image\n</code></pre>"},{"location":"model/#src.model.NSTImageTrainer.__style_content_loss","title":"<code>__style_content_loss(outputs)</code>","text":"<p>Compute the style and content loss based on output from StyleContentExtractor.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>dict[str, dict]</code> <p>output from StyleContentExtractor call.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Total weighted style and content loss.</p> Source code in <code>src/model.py</code> <pre><code>def __style_content_loss(self, outputs: dict[str, dict]) -&gt; float:\n\"\"\"Compute the style and content loss based on output from StyleContentExtractor.\n\n    Args:\n        outputs (dict[str, dict]): output from StyleContentExtractor call.\n\n    Returns:\n        float: Total weighted style and content loss.\n    \"\"\"\n    style_outputs = outputs[\"style\"]\n    content_outputs = outputs[\"content\"]\n    style_loss = tf.add_n(\n        [\n            tf.reduce_mean((style_outputs[name] - self.style_targets[name]) ** 2)\n            for name in style_outputs.keys()\n        ]\n    )\n    style_loss *= self.style_weight / self.num_style_layers\n\n    content_loss = tf.add_n(\n        [\n            tf.reduce_mean(\n                (content_outputs[name] - self.content_targets[name]) ** 2\n            )\n            for name in content_outputs.keys()\n        ]\n    )\n    content_loss *= self.content_weight / self.num_content_layers\n    loss = style_loss + content_loss\n    return loss\n</code></pre>"},{"location":"model/#src.model.NSTImageTrainer.from_layers_selectors","title":"<code>from_layers_selectors(style_image, content_image, style_layers_selector, content_layers_selector, trainer_kw=dict(total_variation_weight=30), style_layers_selector_kw=dict(), content_layers_selector_kw=dict())</code>  <code>classmethod</code>","text":"<p>Create a NSTImageTrainer from a style_layers_selector and content_layler_selector functions.</p> <p>Parameters:</p> Name Type Description Default <code>style_image</code> <code>tf.Tensor</code> <p>3D Tensor represents style image.</p> required <code>content_image</code> <code>tf.Tensor</code> <p>3D Tensor represents content image.</p> required <code>style_layers_selector</code> <code>Callable[[list[str]], list[str]]</code> <p>Function, that takes as arugment list of all convolutional layers names and return subset of them. Returned list will be used as style layers in currently created NSTImageTrainer.</p> required <code>content_layers_selector</code> <code>Callable[[list[str]], list[str]]</code> <p>Function, that takes as arugment list of all convolutional layers names and return subset of them. Returned list will be used as content layers in currently created NSTImageTrainer.</p> required <code>trainer_kw</code> <code>dict</code> <p>Keyword arguments to create NSTImageTrainer. Defaults to {\"total_variation_weight\": 30}.</p> <code>dict(total_variation_weight=30)</code> <code>style_layers_selector_kw</code> <code>[dict]</code> <p>Keywords arguments of style_layers_selector. Defaults to dict().</p> <code>dict()</code> <code>content_layers_selector_kw</code> <code>[dict]</code> <p>Keywords arguments of content_layers_selector. Defaults to dict().</p> <code>dict()</code> <p>Returns:</p> Type Description <p>NSTImageTrainer</p> Source code in <code>src/model.py</code> <pre><code>@classmethod\ndef from_layers_selectors(\n    self,\n    style_image: tf.Tensor,\n    content_image: tf.Tensor,\n    style_layers_selector: Callable[[list[str]], list[str]],\n    content_layers_selector: Callable[[list[str]], list[str]],\n    trainer_kw=dict(total_variation_weight=30),\n    style_layers_selector_kw: dict = dict(),\n    content_layers_selector_kw: dict = dict(),\n):\n\"\"\"Create a NSTImageTrainer from a style_layers_selector and content_layler_selector functions.\n\n    Args:\n        style_image (tf.Tensor): 3D Tensor represents style image.\n        content_image (tf.Tensor): 3D Tensor represents content image.\n        style_layers_selector (Callable[[list[str]], list[str]]): Function, that takes as arugment\n            list of all convolutional layers names and return subset of them. Returned list will be used\n            as style layers in currently created NSTImageTrainer.\n        content_layers_selector (Callable[[list[str]], list[str]]): Function, that takes as arugment\n            list of all convolutional layers names and return subset of them. Returned list will be used\n            as content layers in currently created NSTImageTrainer.\n        trainer_kw (dict, optional): Keyword arguments to create NSTImageTrainer.\n            Defaults to {\"total_variation_weight\": 30}.\n        style_layers_selector_kw ([dict], optional): Keywords arguments of style_layers_selector.\n            Defaults to dict().\n        content_layers_selector_kw ([dict], optional): Keywords arguments of content_layers_selector.\n            Defaults to dict().\n\n    Returns:\n        NSTImageTrainer\n    \"\"\"\n    layers = NSTImageTrainer.model_layers_names()\n    conv_layers = [name for name in layers if \"conv\" in name]\n\n    style_layers = style_layers_selector(conv_layers, **style_layers_selector_kw)\n    content_layers = content_layers_selector(\n        conv_layers, **content_layers_selector_kw\n    )\n\n    trainer = NSTImageTrainer(\n        style_image, content_image, style_layers, content_layers, **trainer_kw\n    )\n    return trainer\n</code></pre>"},{"location":"model/#src.model.NSTImageTrainer.model_layers_names","title":"<code>model_layers_names()</code>  <code>staticmethod</code>","text":"<p>Returns a list of all model layers names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]</p> Source code in <code>src/model.py</code> <pre><code>@staticmethod\ndef model_layers_names() -&gt; list[str]:\n\"\"\"Returns a list of all model layers names.\n\n    Returns:\n        list[str]\n    \"\"\"\n    return [layer.name for layer in StyleContentExtractor.base_model.layers]\n</code></pre>"},{"location":"model/#src.model.NSTImageTrainer.train_step","title":"<code>train_step()</code>","text":"<p>Perform the actual training step.</p> Source code in <code>src/model.py</code> <pre><code>@tf.function()\n@tf.autograph.experimental.do_not_convert\ndef train_step(self) -&gt; None:\n\"\"\"Perform the actual training step.\"\"\"\n    with tf.GradientTape() as tape:\n        outputs = self.extractor(self.__output_image)\n        loss = self.__style_content_loss(outputs)\n        loss += self.__calculate_total_variation()\n\n    grad = tape.gradient(loss, self.__output_image)\n    self.optimizer.apply_gradients([(grad, self.__output_image)])\n    self.__output_image.assign(utils.tf_utils.clip_0_1(self.__output_image))\n</code></pre>"},{"location":"model/#src.model.NSTImageTrainer.training_loop","title":"<code>training_loop(steps_per_epoch, epochs, callbacks=None)</code>","text":"<p>Train output image. After every epoch callbacks are called.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Nuber of epoch in training.</p> required <code>steps</code> <code>int</code> <p>Training steps in every epoch. After each step gradiends are applied to image.</p> required <code>callbacks</code> <code>list[Callable[[]]]</code> <p>List of callable objects,</p> <code>None</code> <p>Callback wrap examples:</p> <p>def callback_without_arg():     callback(global_arg1, global_arg2)</p> <p>or</p> class CallbackObj <p>def init(arg1, arg2, callback):     self.arg1 = arg1     self.arg2 = arg2     self.callback = callback</p> <p>def call(self):     self.callback(self.arg1, self.arg2)</p> Source code in <code>src/model.py</code> <pre><code>def training_loop(\n    self,\n    steps_per_epoch: int,\n    epochs: int,\n    callbacks: list[Callable[[], None]] = None,\n) -&gt; None:\n\"\"\"Train output image. After every epoch callbacks are called.\n\n    Args:\n        epochs (int): Nuber of epoch in training.\n        steps (int): Training steps in every epoch. After each step gradiends are applied to image.\n        callbacks (list[Callable[[],]], optional): List of callable objects,\n        that are called on the end of every trainer training.\n        They cannot take any parameters. If you need to use function with parameters,\n        wrap it like in exapmle below. Defaults to None.\n\n    Callback wrap examples:\n\n    def callback_without_arg():\n        callback(global_arg1, global_arg2)\n\n    or\n\n    class CallbackObj:\n        def __init__(arg1, arg2, callback):\n            self.arg1 = arg1\n            self.arg2 = arg2\n            self.callback = callback\n\n        def __call__(self):\n            self.callback(self.arg1, self.arg2)\n    \"\"\"\n    for epoch_num in range(1, epochs + 1):\n        print(f\"Epoch {epoch_num}/{epochs}:\")\n        for _ in tqdm(range(steps_per_epoch)):\n            self.train_step()\n        if callbacks:\n            for callback in callbacks:\n                callback()\n</code></pre>"},{"location":"model/#src.model.StyleContentExtractor","title":"<code>StyleContentExtractor</code>","text":"<p>         Bases: <code>tf.keras.models.Model</code></p> Source code in <code>src/model.py</code> <pre><code>class StyleContentExtractor(tf.keras.models.Model):\n    base_model = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n    base_model.trainable = False\n\n    def __init__(\n        self, style_layers: list[str], content_layers: list[str], *args, **kwargs\n    ) -&gt; None:\n\"\"\"StyleContentExtractor implements forward pass of NST image training. It is core of NSTImageTrainer.\n            All created StyleContentExtractors share the same VGG19 model as base, to save memory.\n\n        Args:\n            style_layers (list[str]): list of all vgg layers names used to calculate style.\n                List of all avaliable layers can be returnrd by NSTImageTrainer.model_layers_names()\n            content_layers (list[str]): list of all vgg layers names used to calculate content\n                (Usualy one layer is recomended).\n                List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names()\n\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.model = self.__init_model(style_layers + content_layers)\n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.num_style_layers = len(style_layers)\n\n    def call(self, inputs: tf.Tensor, input_max_value: int = 1) -&gt; dict[str, dict]:\n\"\"\"Implement forward pass of the model.\n\n        Args:\n            inputs (tf.Tensor): Tensor with image/images batch to process\n            input_max_value (int, optional): Maximal value in input image. Defaults to 1.\n\n        Returns:\n            [dict[str, dict]]: dict with two keys: \"content\" and \"style\",\n            value of each key contains neasted dict with layer name as key\n            and corespoding model output as value\n        \"\"\"\n        content_outputs, style_outputs = self.__get_model_outputs(\n            inputs, input_max_value\n        )\n        content_dict = dict(zip(self.content_layers, content_outputs))\n        style_dict = dict(zip(self.style_layers, style_outputs))\n        return {\"content\": content_dict, \"style\": style_dict}\n\n    def __init_model(self, layer_names: list[str]):\n\"\"\"Initialize vgg model with outputs specified in layers_names.\n\n        Args:\n            layer_names (list[str]): Both, style and content layers used to define output layers of model.\n\n        Returns:\n            tf.keras.Model: Vgg19 model with multiple outputs on diffrent layers.\n        \"\"\"\n        outputs = [self.base_model.get_layer(name).output for name in layer_names]\n        return tf.keras.Model([self.base_model.input], outputs)\n\n    def __get_model_outputs(\n        self, inputs: tf.Tensor, input_max_value: int\n    ) -&gt; tuple[list[tf.Tensor], list[tf.Tensor]]:\n\"\"\"Runs the model and returns the content and style matrices.\n\n        Args:\n            inputs (tf.Tensor): Tensor with image/images batch to process\n            input_max_value (int, optional): Maximal value in input image. Defaults to 1.\n\n        Returns:\n            [tuple[list[tf.Tensor], list[tf.Tensor]]]: Two lists with content and style model outputs.\n        \"\"\"\n        processed_input = self.__preprocess_input(inputs, input_max_value)\n        outputs = self.model(processed_input)\n        content_outputs = outputs[self.num_style_layers :]\n        style_outputs = map(\n            utils.tf_utils.gram_matrix, outputs[: self.num_style_layers]\n        )\n        return content_outputs, style_outputs\n\n    def __preprocess_input(self, inputs: tf.Tensor, input_max_value) -&gt; tf.Tensor:\n\"\"\"Preprocess the input to meet vgg requirements.\n\n        Args:\n            inputs (tf.Tensor): Tensor with image/images batch to process.\n            input_max_value (int, optional): Maximal value in input image. Defaults to 1.\n\n        Returns:\n            tf.Tensor: Preprocessed image\n        \"\"\"\n        # self.__assert_max_value_not_exceeded(inputs, input_max_value)\n        inputs = inputs * (255.0 / input_max_value)\n        return tf.keras.applications.vgg19.preprocess_input(inputs)\n\n    # def __assert_max_value_not_exceeded(self, inputs, input_max_value):\n    #     biggest_input = tf.reduce_max(inputs)\n    #     if biggest_input &gt; input_max_value:\n    #         raise ValueError(\n    #             f\"Given tensor have values grater than {input_max_value = }, {biggest_input = }\"\n    #         )\n\n    @classmethod\n    def model_layers_names(cls) -&gt; list[str]:\n\"\"\"Returns a list of vgg19 layer names.\n\n        Returns:\n            list[str]\n        \"\"\"\n        return [layer.name for layer in cls.base_model.layers]\n</code></pre>"},{"location":"model/#src.model.StyleContentExtractor.__get_model_outputs","title":"<code>__get_model_outputs(inputs, input_max_value)</code>","text":"<p>Runs the model and returns the content and style matrices.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>Tensor with image/images batch to process</p> required <code>input_max_value</code> <code>int</code> <p>Maximal value in input image. Defaults to 1.</p> required <p>Returns:</p> Type Description <code>tuple[list[tf.Tensor], list[tf.Tensor]]</code> <p>[tuple[list[tf.Tensor], list[tf.Tensor]]]: Two lists with content and style model outputs.</p> Source code in <code>src/model.py</code> <pre><code>def __get_model_outputs(\n    self, inputs: tf.Tensor, input_max_value: int\n) -&gt; tuple[list[tf.Tensor], list[tf.Tensor]]:\n\"\"\"Runs the model and returns the content and style matrices.\n\n    Args:\n        inputs (tf.Tensor): Tensor with image/images batch to process\n        input_max_value (int, optional): Maximal value in input image. Defaults to 1.\n\n    Returns:\n        [tuple[list[tf.Tensor], list[tf.Tensor]]]: Two lists with content and style model outputs.\n    \"\"\"\n    processed_input = self.__preprocess_input(inputs, input_max_value)\n    outputs = self.model(processed_input)\n    content_outputs = outputs[self.num_style_layers :]\n    style_outputs = map(\n        utils.tf_utils.gram_matrix, outputs[: self.num_style_layers]\n    )\n    return content_outputs, style_outputs\n</code></pre>"},{"location":"model/#src.model.StyleContentExtractor.__init__","title":"<code>__init__(style_layers, content_layers, *args, **kwargs)</code>","text":"<p>StyleContentExtractor implements forward pass of NST image training. It is core of NSTImageTrainer.     All created StyleContentExtractors share the same VGG19 model as base, to save memory.</p> <p>Parameters:</p> Name Type Description Default <code>style_layers</code> <code>list[str]</code> <p>list of all vgg layers names used to calculate style. List of all avaliable layers can be returnrd by NSTImageTrainer.model_layers_names()</p> required <code>content_layers</code> <code>list[str]</code> <p>list of all vgg layers names used to calculate content (Usualy one layer is recomended). List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names()</p> required Source code in <code>src/model.py</code> <pre><code>def __init__(\n    self, style_layers: list[str], content_layers: list[str], *args, **kwargs\n) -&gt; None:\n\"\"\"StyleContentExtractor implements forward pass of NST image training. It is core of NSTImageTrainer.\n        All created StyleContentExtractors share the same VGG19 model as base, to save memory.\n\n    Args:\n        style_layers (list[str]): list of all vgg layers names used to calculate style.\n            List of all avaliable layers can be returnrd by NSTImageTrainer.model_layers_names()\n        content_layers (list[str]): list of all vgg layers names used to calculate content\n            (Usualy one layer is recomended).\n            List of all avaliable layers can be returned by NSTImageTrainer.model_layers_names()\n\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.model = self.__init_model(style_layers + content_layers)\n    self.style_layers = style_layers\n    self.content_layers = content_layers\n    self.num_style_layers = len(style_layers)\n</code></pre>"},{"location":"model/#src.model.StyleContentExtractor.__init_model","title":"<code>__init_model(layer_names)</code>","text":"<p>Initialize vgg model with outputs specified in layers_names.</p> <p>Parameters:</p> Name Type Description Default <code>layer_names</code> <code>list[str]</code> <p>Both, style and content layers used to define output layers of model.</p> required <p>Returns:</p> Type Description <p>tf.keras.Model: Vgg19 model with multiple outputs on diffrent layers.</p> Source code in <code>src/model.py</code> <pre><code>def __init_model(self, layer_names: list[str]):\n\"\"\"Initialize vgg model with outputs specified in layers_names.\n\n    Args:\n        layer_names (list[str]): Both, style and content layers used to define output layers of model.\n\n    Returns:\n        tf.keras.Model: Vgg19 model with multiple outputs on diffrent layers.\n    \"\"\"\n    outputs = [self.base_model.get_layer(name).output for name in layer_names]\n    return tf.keras.Model([self.base_model.input], outputs)\n</code></pre>"},{"location":"model/#src.model.StyleContentExtractor.__preprocess_input","title":"<code>__preprocess_input(inputs, input_max_value)</code>","text":"<p>Preprocess the input to meet vgg requirements.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>Tensor with image/images batch to process.</p> required <code>input_max_value</code> <code>int</code> <p>Maximal value in input image. Defaults to 1.</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>tf.Tensor: Preprocessed image</p> Source code in <code>src/model.py</code> <pre><code>def __preprocess_input(self, inputs: tf.Tensor, input_max_value) -&gt; tf.Tensor:\n\"\"\"Preprocess the input to meet vgg requirements.\n\n    Args:\n        inputs (tf.Tensor): Tensor with image/images batch to process.\n        input_max_value (int, optional): Maximal value in input image. Defaults to 1.\n\n    Returns:\n        tf.Tensor: Preprocessed image\n    \"\"\"\n    # self.__assert_max_value_not_exceeded(inputs, input_max_value)\n    inputs = inputs * (255.0 / input_max_value)\n    return tf.keras.applications.vgg19.preprocess_input(inputs)\n</code></pre>"},{"location":"model/#src.model.StyleContentExtractor.call","title":"<code>call(inputs, input_max_value=1)</code>","text":"<p>Implement forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>Tensor with image/images batch to process</p> required <code>input_max_value</code> <code>int</code> <p>Maximal value in input image. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>[dict[str, dict]]: dict with two keys: \"content\" and \"style\",</p> <code>dict[str, dict]</code> <p>value of each key contains neasted dict with layer name as key</p> <code>dict[str, dict]</code> <p>and corespoding model output as value</p> Source code in <code>src/model.py</code> <pre><code>def call(self, inputs: tf.Tensor, input_max_value: int = 1) -&gt; dict[str, dict]:\n\"\"\"Implement forward pass of the model.\n\n    Args:\n        inputs (tf.Tensor): Tensor with image/images batch to process\n        input_max_value (int, optional): Maximal value in input image. Defaults to 1.\n\n    Returns:\n        [dict[str, dict]]: dict with two keys: \"content\" and \"style\",\n        value of each key contains neasted dict with layer name as key\n        and corespoding model output as value\n    \"\"\"\n    content_outputs, style_outputs = self.__get_model_outputs(\n        inputs, input_max_value\n    )\n    content_dict = dict(zip(self.content_layers, content_outputs))\n    style_dict = dict(zip(self.style_layers, style_outputs))\n    return {\"content\": content_dict, \"style\": style_dict}\n</code></pre>"},{"location":"model/#src.model.StyleContentExtractor.model_layers_names","title":"<code>model_layers_names()</code>  <code>classmethod</code>","text":"<p>Returns a list of vgg19 layer names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]</p> Source code in <code>src/model.py</code> <pre><code>@classmethod\ndef model_layers_names(cls) -&gt; list[str]:\n\"\"\"Returns a list of vgg19 layer names.\n\n    Returns:\n        list[str]\n    \"\"\"\n    return [layer.name for layer in cls.base_model.layers]\n</code></pre>"},{"location":"selectioner/","title":"Selectioner","text":"<p><code>selectioner.py</code> contains <code>TraintersSelectioner</code> definition. This class is designed to train and select best <code>NSTImageTrainers</code> automatically for you.</p>"},{"location":"selectioner/#src.selectioner.TraintersSelectioner","title":"<code>TraintersSelectioner</code>","text":"Source code in <code>src/selectioner.py</code> <pre><code>class TraintersSelectioner:\n    def __init__(self, trainers: list) -&gt; None:\n\"\"\"TraintersSelectioner provides methods, that can help in, or automate,\n            process of choosing best hyperparameters for NSTImageTrainers.\n\n        Args:\n            trainers (list[NSTImageTrainer]): List of trainers to select from.\n        \"\"\"\n        self.__trainers = pd.Series(trainers)\n        self.history = []\n\n    def train(\n        self,\n        epochs: int,\n        steps: int,\n        callbacks: list[Callable[[], None]] = None,\n    ) -&gt; None:\n\"\"\"Train all the NSTImageTrainers. After every trainer training callbacks are called.\n\n        Args:\n            epochs (int): Nuber of epoch to train for single trainer.\n            steps (int): Training steps in every epoch. After each step gradiends are applied.\n            callbacks (list[Callable[[],]], optional): List of callable objects,\n                that are called at the end of every trainer training.\n                They cannot take any parameters. If you need to use function with parameters,\n                wrap it like in exapmle below. Defaults to None.\n\n        Callback wrap examples:\n\n        def callback_without_arg():\n            callback(global_arg1, global_arg2)\n\n        or\n\n        class CallbackObj:\n            def __init__(arg1, arg2, callback):\n                self.arg1 = arg1\n                self.arg2 = arg2\n                self.callback = callback\n\n            def __call__(self):\n                self.callback(self.arg1, self.arg2)\n        \"\"\"\n        for i, trainer in enumerate(self.trainers):\n            print(f\"Traing for trainer {i+1} from {len(self.trainers)}\")\n            trainer.training_loop(epochs=epochs, steps_per_epoch=steps)\n            if callbacks:\n                for callback in callbacks:\n                    callback()\n\n    def save_history(self) -&gt; None:\n\"\"\"Save history of trainers. Be aware that trainers aren't copied,\n        so in practice only current trainers order is saved, \n        in order to check what happend with particular trainer,\n        or when you don't need to worry about removing trainers.\n        \"\"\"\n        self.history.append(self.__trainers)\n\n    def sort_trainers_by_differences(\n        self, ordering_method: Callable[[tf.Tensor, tf.Tensor], float]\n    ) -&gt; None:\n\"\"\"Apply ordering_method to all pairs of trained images, and average results per trainer,\n            after that, sorts trainers in descending order, based on those averages.\n\n        Args:\n            ordering_method (Callable[[tf.Tensor, tf.Tensor], float]): Function that input two 3D tensors\n                and return one number. The higher the number, \n                the earlier the trainers colerated with images will be in trainers list.\n        \"\"\"\n        t1, t2 = zip(*itertools.product(self.__trainers, self.__trainers))\n        df = pd.DataFrame([t1, t2]).transpose()\n        df.columns = [\"t1\", \"t2\"]\n        df[\"dst\"] = df.apply(\n            lambda x: ordering_method(\n                np.array(x[0].output_image), np.array(x[1].output_image)\n            ),\n            axis=1,\n        )\n        df = df.set_index([\"t1\", \"t2\"]).unstack()\n        df = df.mean(axis=1).sort_values(ascending=False)\n        self.__trainers = list(df.index)\n\n    def remove_second_half_trainers(self) -&gt; None:\n\"\"\"Removes the second half of trainers from the training list.\n        Typiacally used after sorting trainers by differences.\n        \"\"\"\n        self.__trainers = self.__trainers[: len(self.trainers) // 2]\n\n    @property\n    def trainers(self) -&gt; list:\n\"\"\"The list of all active trainers.\n\n        Returns:\n            list[NSTImageTrainers]\n        \"\"\"\n        return self.__trainers\n</code></pre>"},{"location":"selectioner/#src.selectioner.TraintersSelectioner.trainers","title":"<code>trainers: list</code>  <code>property</code>","text":"<p>The list of all active trainers.</p> <p>Returns:</p> Type Description <code>list</code> <p>list[NSTImageTrainers]</p>"},{"location":"selectioner/#src.selectioner.TraintersSelectioner.__init__","title":"<code>__init__(trainers)</code>","text":"<p>TraintersSelectioner provides methods, that can help in, or automate,     process of choosing best hyperparameters for NSTImageTrainers.</p> <p>Parameters:</p> Name Type Description Default <code>trainers</code> <code>list[NSTImageTrainer]</code> <p>List of trainers to select from.</p> required Source code in <code>src/selectioner.py</code> <pre><code>def __init__(self, trainers: list) -&gt; None:\n\"\"\"TraintersSelectioner provides methods, that can help in, or automate,\n        process of choosing best hyperparameters for NSTImageTrainers.\n\n    Args:\n        trainers (list[NSTImageTrainer]): List of trainers to select from.\n    \"\"\"\n    self.__trainers = pd.Series(trainers)\n    self.history = []\n</code></pre>"},{"location":"selectioner/#src.selectioner.TraintersSelectioner.remove_second_half_trainers","title":"<code>remove_second_half_trainers()</code>","text":"<p>Removes the second half of trainers from the training list. Typiacally used after sorting trainers by differences.</p> Source code in <code>src/selectioner.py</code> <pre><code>def remove_second_half_trainers(self) -&gt; None:\n\"\"\"Removes the second half of trainers from the training list.\n    Typiacally used after sorting trainers by differences.\n    \"\"\"\n    self.__trainers = self.__trainers[: len(self.trainers) // 2]\n</code></pre>"},{"location":"selectioner/#src.selectioner.TraintersSelectioner.save_history","title":"<code>save_history()</code>","text":"<p>Save history of trainers. Be aware that trainers aren't copied, so in practice only current trainers order is saved,  in order to check what happend with particular trainer, or when you don't need to worry about removing trainers.</p> Source code in <code>src/selectioner.py</code> <pre><code>def save_history(self) -&gt; None:\n\"\"\"Save history of trainers. Be aware that trainers aren't copied,\n    so in practice only current trainers order is saved, \n    in order to check what happend with particular trainer,\n    or when you don't need to worry about removing trainers.\n    \"\"\"\n    self.history.append(self.__trainers)\n</code></pre>"},{"location":"selectioner/#src.selectioner.TraintersSelectioner.sort_trainers_by_differences","title":"<code>sort_trainers_by_differences(ordering_method)</code>","text":"<p>Apply ordering_method to all pairs of trained images, and average results per trainer,     after that, sorts trainers in descending order, based on those averages.</p> <p>Parameters:</p> Name Type Description Default <code>ordering_method</code> <code>Callable[[tf.Tensor, tf.Tensor], float]</code> <p>Function that input two 3D tensors and return one number. The higher the number,  the earlier the trainers colerated with images will be in trainers list.</p> required Source code in <code>src/selectioner.py</code> <pre><code>def sort_trainers_by_differences(\n    self, ordering_method: Callable[[tf.Tensor, tf.Tensor], float]\n) -&gt; None:\n\"\"\"Apply ordering_method to all pairs of trained images, and average results per trainer,\n        after that, sorts trainers in descending order, based on those averages.\n\n    Args:\n        ordering_method (Callable[[tf.Tensor, tf.Tensor], float]): Function that input two 3D tensors\n            and return one number. The higher the number, \n            the earlier the trainers colerated with images will be in trainers list.\n    \"\"\"\n    t1, t2 = zip(*itertools.product(self.__trainers, self.__trainers))\n    df = pd.DataFrame([t1, t2]).transpose()\n    df.columns = [\"t1\", \"t2\"]\n    df[\"dst\"] = df.apply(\n        lambda x: ordering_method(\n            np.array(x[0].output_image), np.array(x[1].output_image)\n        ),\n        axis=1,\n    )\n    df = df.set_index([\"t1\", \"t2\"]).unstack()\n    df = df.mean(axis=1).sort_values(ascending=False)\n    self.__trainers = list(df.index)\n</code></pre>"},{"location":"selectioner/#src.selectioner.TraintersSelectioner.train","title":"<code>train(epochs, steps, callbacks=None)</code>","text":"<p>Train all the NSTImageTrainers. After every trainer training callbacks are called.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Nuber of epoch to train for single trainer.</p> required <code>steps</code> <code>int</code> <p>Training steps in every epoch. After each step gradiends are applied.</p> required <code>callbacks</code> <code>list[Callable[[]]]</code> <p>List of callable objects, that are called at the end of every trainer training. They cannot take any parameters. If you need to use function with parameters, wrap it like in exapmle below. Defaults to None.</p> <code>None</code> <p>Callback wrap examples:</p> <p>def callback_without_arg():     callback(global_arg1, global_arg2)</p> <p>or</p> class CallbackObj <p>def init(arg1, arg2, callback):     self.arg1 = arg1     self.arg2 = arg2     self.callback = callback</p> <p>def call(self):     self.callback(self.arg1, self.arg2)</p> Source code in <code>src/selectioner.py</code> <pre><code>def train(\n    self,\n    epochs: int,\n    steps: int,\n    callbacks: list[Callable[[], None]] = None,\n) -&gt; None:\n\"\"\"Train all the NSTImageTrainers. After every trainer training callbacks are called.\n\n    Args:\n        epochs (int): Nuber of epoch to train for single trainer.\n        steps (int): Training steps in every epoch. After each step gradiends are applied.\n        callbacks (list[Callable[[],]], optional): List of callable objects,\n            that are called at the end of every trainer training.\n            They cannot take any parameters. If you need to use function with parameters,\n            wrap it like in exapmle below. Defaults to None.\n\n    Callback wrap examples:\n\n    def callback_without_arg():\n        callback(global_arg1, global_arg2)\n\n    or\n\n    class CallbackObj:\n        def __init__(arg1, arg2, callback):\n            self.arg1 = arg1\n            self.arg2 = arg2\n            self.callback = callback\n\n        def __call__(self):\n            self.callback(self.arg1, self.arg2)\n    \"\"\"\n    for i, trainer in enumerate(self.trainers):\n        print(f\"Traing for trainer {i+1} from {len(self.trainers)}\")\n        trainer.training_loop(epochs=epochs, steps_per_epoch=steps)\n        if callbacks:\n            for callback in callbacks:\n                callback()\n</code></pre>"},{"location":"structure/","title":"Project Structure","text":"<p>The project structure is shown on schema below. You can familiarize yourself with project modules by reading short description on the end of each. Some of schema modules are bound to their documentation, so you can learn more about them by clicking in it. Every documentation start with brief introduction to module.</p> <p> \u251c\u2500\u2500 README.md \u251c\u2500\u2500 images/ stores input and output images \u2502   \u251c\u2500\u2500 contents/ stores content images \u2502   \u251c\u2500\u2500 styles/ stores style images \u2502   \u251c\u2500\u2500 results/ stores generated images \u2502   \u2514\u2500\u2500 trainers/ stores trainers visualizations \u251c\u2500\u2500 src/ contains project source code \u2502   \u251c\u2500\u2500 utils/ contains helper functions \u2502   \u2502   \u251c\u2500\u2500 __init__.py makes python treat utils as package \u2502   \u2502   \u251c\u2500\u2500 randomizers.py contains random trainer init helpers \u2502   \u2502   \u2514\u2500\u2500 tf_utils.py contains model.py helpers \u2502   \u251c\u2500\u2500 model.py NST model source code \u2502   \u251c\u2500\u2500 selectioner.py automatically chooses best NST models \u2502   \u2514\u2500\u2500 visualization.py deals with plots \u251c\u2500\u2500 requirements/ stores configuration files for python env \u2502   \u251c\u2500\u2500 environment.yaml/ defines conda env \u2502   \u251c\u2500\u2500 pip_requirements.txt/ defines pip env \u2502   \u2514\u2500\u2500 raw_requirements.txt/ defines pip env without version specified \u251c\u2500\u2500 nst_training.ipynb demonstrates simple image generation process \u251c\u2500\u2500 manual_training.ipynb demonstates harder image generation process \u251c\u2500\u2500 mkdocs.yml documentation config \u2514\u2500\u2500 .gitignore defines files ignored by git </p>"},{"location":"utils/","title":"Utils","text":"<p>There are two files in utils:  </p> <ul> <li> <p><code>randomizers.py</code> contains functions that may be used in <code>NSTImageTrainer.from_layers_selectors()</code> for picking layers used in training.</p> </li> <li> <p><code>tf_utils.py</code> contains tensorflow helper functions that are used in model.py</p> </li> </ul>"},{"location":"utils/#randomizerspy","title":"randomizers.py","text":""},{"location":"utils/#src.utils.randomizers.normal_choice","title":"<code>normal_choice(data, rel_loc=0.5, rel_scale=0.5)</code>","text":"<p>Weighted random choice of element from list. Weights are generated from normal distribution.     Loc of the normal distribution is equal to rel_loc * number of elements in data.     Analogous scale is equal to rel_scale * number of elements in data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[any]</code> <p>Data collection to choose from.</p> required <code>rel_loc</code> <code>float</code> <p>Relative loc of normal distribution.  True loc is calculate based on that with the formula:  loc = rel_loc * number of elements in data. Defaults to 0.5.</p> <code>0.5</code> <code>rel_scale</code> <code>float</code> <p>Relative scale of normal distribution.  True scale is calculate based on that with the formula:  scale = rel_scale * number of elements in data. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>list[any]</code> <p>list[any]: List with single choosen element.</p> Source code in <code>src/utils/randomizers.py</code> <pre><code>def normal_choice(data: list[any], rel_loc: float=0.5, rel_scale: float=0.5) -&gt; list[any]:  # 0.5, 0.26\n\"\"\"Weighted random choice of element from list. Weights are generated from normal distribution.\n        Loc of the normal distribution is equal to rel_loc * number of elements in data.\n        Analogous scale is equal to rel_scale * number of elements in data.\n\n    Args:\n        data (list[any]): Data collection to choose from.\n        rel_loc (float, optional): Relative loc of normal distribution. \n            True loc is calculate based on that with the formula: \n            loc = rel_loc * number of elements in data. Defaults to 0.5.\n        rel_scale (float, optional): Relative scale of normal distribution. \n            True scale is calculate based on that with the formula: \n            scale = rel_scale * number of elements in data. Defaults to 0.5.\n    Returns:\n        list[any]: List with single choosen element.\n    \"\"\"\n    elements_count = len(data)\n\n    elements_indexes = range(elements_count)\n    elements_weights = scipy.stats.norm.pdf(\n        elements_indexes, loc=elements_count * rel_loc, scale=elements_count * rel_scale\n    )\n    elements_weights /= sum(elements_weights)\n    return random.choices(data, weights=elements_weights, k=1)\n</code></pre>"},{"location":"utils/#src.utils.randomizers.random_length_choices","title":"<code>random_length_choices(data, min_output_elements_num=1, rel_loc=0.5, rel_scale=0.5)</code>","text":"<p>Get subset of data, uniformly sampled, with sample size sampled from normal dystribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[any]</code> <p>Data collection to choose from.</p> required <code>min_output_elements_num</code> <code>int</code> <p>Minimal number of of output elments, that needs to be satisfied. Defaults to 1.</p> <code>1</code> <code>rel_loc</code> <code>float</code> <p>Relative loc of normal distribution,  used to sample sample size.  True loc is calculate based on that with the formula:  loc = rel_loc * number of elements in data. Defaults to 0.5.</p> <code>0.5</code> <code>rel_scale</code> <code>float</code> <p>Relative scale of normal distribution,  used to sample sample size.  True scale is calculate based on that with the formula:  scale = rel_scale * number of elements in data. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>list[any]</code> <p>list[any]: List with choosen elements.</p> Source code in <code>src/utils/randomizers.py</code> <pre><code>def random_length_choices(\n    data: list[any],  \n    min_output_elements_num: int=1, \n    rel_loc: float=0.5, \n    rel_scale: float=0.5\n    ) -&gt; list[any]:  # 4, 0.05, 0.25\n\"\"\"Get subset of data, uniformly sampled, with sample size sampled from normal dystribution.\n\n    Args:\n        data (list[any]): Data collection to choose from.\n        min_output_elements_num (int, optional): Minimal number of of output elments,\n            that needs to be satisfied. Defaults to 1.\n        rel_loc (float, optional): Relative loc of normal distribution, \n            used to sample sample size. \n            True loc is calculate based on that with the formula: \n            loc = rel_loc * number of elements in data. Defaults to 0.5.\n        rel_scale (float, optional): Relative scale of normal distribution, \n            used to sample sample size. \n            True scale is calculate based on that with the formula: \n            scale = rel_scale * number of elements in data. Defaults to 0.5.\n\n    Returns:\n        list[any]: List with choosen elements.\n    \"\"\"\n    elements_count = len(data)\n    elements_indexes = range(elements_count - min_output_elements_num)\n    elements_weights = scipy.stats.norm.pdf(\n        elements_indexes,\n        loc=(elements_count - min_output_elements_num) * rel_loc,\n        scale=elements_count * rel_scale,\n    )\n    elements_weights /= sum(elements_weights)\n    elements_weights = np.cumsum(elements_weights)\n    output_elements_num = (\n        random.random() &gt; elements_weights\n    ).sum() + min_output_elements_num\n    output_elements = random.sample(data, k=output_elements_num)\n    output_elements.sort()\n    return output_elements\n</code></pre>"},{"location":"utils/#tf_utilspy","title":"tf_utils.py","text":""},{"location":"utils/#src.utils.tf_utils.clip_0_1","title":"<code>clip_0_1(image)</code>","text":"<p>Clips tensor values to values from range 0 to 1.</p> Source code in <code>src/utils/tf_utils.py</code> <pre><code>def clip_0_1(image):\n\"\"\"Clips tensor values to values from range 0 to 1.\n    \"\"\"\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n</code></pre>"},{"location":"utils/#src.utils.tf_utils.gram_matrix","title":"<code>gram_matrix(input_tensor)</code>","text":"<p>Compute the gram matrix of a input_tensor.    Apply to dimensions like: bijc,bijd-&gt;bcd</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>tf.Tensor</code> <p>4D tensor</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>3D tensor</p> Source code in <code>src/utils/tf_utils.py</code> <pre><code>def gram_matrix(input_tensor: tf.Tensor) -&gt; float:\n\"\"\"Compute the gram matrix of a input_tensor.\n       Apply to dimensions like: bijc,bijd-&gt;bcd\n\n    Args:\n        input_tensor (tf.Tensor): 4D tensor\n\n    Returns:\n        float: 3D tensor\n    \"\"\"\n    result = tf.linalg.einsum(\"bijc,bijd-&gt;bcd\", input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n    return result / (num_locations)\n</code></pre>"},{"location":"utils/#src.utils.tf_utils.load_img","title":"<code>load_img(path_to_img)</code>","text":"<p>Loads an image from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_img</code> <code>str</code> <p>Path where image is stored</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>tf.Tensor: 3D tensor reprezentation of image.</p> Source code in <code>src/utils/tf_utils.py</code> <pre><code>def load_img(path_to_img: str) -&gt; tf.Tensor:\n\"\"\"Loads an image from disk.\n\n    Args:\n        path_to_img (str): Path where image is stored\n\n    Returns:\n        tf.Tensor: 3D tensor reprezentation of image.\n    \"\"\"\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim / long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n</code></pre>"},{"location":"utils/#src.utils.tf_utils.tensor_to_image","title":"<code>tensor_to_image(tensor)</code>","text":"<p>Converts a tensor to PIL Image.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>tf.Tensor</code> <p>3D tensor reprezentation of image.</p> required <p>Returns:</p> Type Description <p>PIL.Image: Image created from tensor.</p> Source code in <code>src/utils/tf_utils.py</code> <pre><code>def tensor_to_image(tensor: tf.Tensor):\n\"\"\"Converts a tensor to PIL Image.\n\n    Args:\n        tensor (tf.Tensor): 3D tensor reprezentation of image.\n\n    Returns:\n        PIL.Image: Image created from tensor.\n    \"\"\"\n    tensor = tensor * 255\n    tensor = np.array(tensor, dtype=np.uint8)\n    if np.ndim(tensor) &gt; 3:\n        assert tensor.shape[0] == 1\n        tensor = tensor[0]\n    return PIL.Image.fromarray(tensor)\n</code></pre>"},{"location":"vizualization/","title":"Vizualization","text":"<p>Contains all functions that display or save images and plots used in this project.</p>"},{"location":"vizualization/#src.vizualization.__find_unique_postfix","title":"<code>__find_unique_postfix(path)</code>","text":"<p>Find a unique postfix to given path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Existing path to file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>[str]: Postfix in \"__([0-9]+)\" format.</p> Source code in <code>src/vizualization.py</code> <pre><code>def __find_unique_postfix(path: str) -&gt; str:\n\"\"\"Find a unique postfix to given path.\n\n    Args:\n        path (str): Existing path to file.\n\n    Returns:\n        [str]: Postfix in \"__([0-9]+)\" format.\n    \"\"\"\n    filename, extension = os.path.splitext(path)\n    idx = 1\n    new_filename = f\"{filename}__({idx})\"\n    while os.path.exists(f\"{new_filename}{extension}\"):\n        idx += 1\n        new_filename = f\"{filename}__({idx})\"\n    return f\"__({idx})\"\n</code></pre>"},{"location":"vizualization/#src.vizualization.get_sublots_dims","title":"<code>get_sublots_dims(subplots_num)</code>","text":"<p>Determine how to arrange number of subplots into grid.     Return number of rows, and columns in this grid.</p> <p>Parameters:</p> Name Type Description Default <code>subplots_num</code> <code>int</code> <p>Number of subplots to arrange</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Triger if program will try to find dividor of subplots_num,  that is lower than 0</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>tuple[int, int]: numbers of rows and columns to arrange subplots</p> Source code in <code>src/vizualization.py</code> <pre><code>def get_sublots_dims(subplots_num: int) -&gt; tuple[int, int]:\n\"\"\"Determine how to arrange number of subplots into grid.\n        Return number of rows, and columns in this grid.\n    Args:\n        subplots_num (int): Number of subplots to arrange\n\n    Raises:\n        RuntimeError: Triger if program will try to find dividor of subplots_num, \n            that is lower than 0\n\n    Returns:\n        tuple[int, int]: numbers of rows and columns to arrange subplots\n    \"\"\"\n    potential_col_num = isqrt(subplots_num)\n\n    while subplots_num % potential_col_num != 0:\n        potential_col_num -= 1\n        if potential_col_num &lt; 0:\n            raise RuntimeError(\"Traing to find dividor lower than 0\")\n    col_num = potential_col_num\n    row_num = int(subplots_num / col_num)\n    return row_num, col_num\n</code></pre>"},{"location":"vizualization/#src.vizualization.plot_trained_images","title":"<code>plot_trained_images(trainers)</code>","text":"<p>Plots the training images for each trainer.</p> <p>Parameters:</p> Name Type Description Default <code>trainers</code> <code>list</code> <p>list of objects with output_image atribute.</p> required Source code in <code>src/vizualization.py</code> <pre><code>def plot_trained_images(trainers: list) -&gt; None:\n\"\"\"Plots the training images for each trainer.\n\n    Args:\n        trainers (list): list of objects with output_image atribute.\n    \"\"\"\n    sublots_dims = get_sublots_dims(len(trainers))\n    subplot_size = 5\n\n    fig, axs = plt.subplots(*sublots_dims, squeeze=True)\n    axs = axs.reshape(-1)\n    fig.set_size_inches(sublots_dims[0] * subplot_size, sublots_dims[1] * subplot_size)\n\n    for i, (trainer, ax) in enumerate(zip(trainers, axs)):\n        plt.sca(ax)\n        plt.axis(\"off\")\n        plt.title(f\"Trainer {i}\")\n        plt.imshow(trainer.output_image)\n</code></pre>"},{"location":"vizualization/#src.vizualization.plot_trainer","title":"<code>plot_trainer(trainer)</code>","text":"<p>Plots most important information about trainer on single image.      That is output, style and content image, as well as layers used during the training.     Style layers are blue. Content layers are red.      If one layer represents style as well content, then it is highlighted on blue with red border. </p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>NSTImageTrainer</code> <p>NSTImageTrainer to vizualize.</p> required Source code in <code>src/vizualization.py</code> <pre><code>def plot_trainer(trainer):\n\"\"\"Plots most important information about trainer on single image. \n        That is output, style and content image, as well as layers used during the training.\n        Style layers are blue. Content layers are red. \n        If one layer represents style as well content, then it is highlighted on blue with red border. \n\n    Args:\n        trainer (NSTImageTrainer): NSTImageTrainer to vizualize.\n    \"\"\"\n    all_model_layers = trainer.model_layers_names()\n    style_layers = trainer.style_layers\n    content_layers = trainer.content_layers\n    fig = plt.gcf()\n    fig.set_size_inches(15, 10)\n    gs = mpl.gridspec.GridSpec(4, 3, width_ratios=(4, 1, 4), height_ratios=(5, 1, 5, 5))\n\n    ax_style = fig.add_subplot(gs[0, 0])\n    ax_content = fig.add_subplot(gs[2, 0])\n    ax_output = fig.add_subplot(gs[0:3, 2])\n    ax_model = fig.add_subplot(gs[3, :])\n\n    ax_plus = fig.add_subplot(gs[1, 0])\n    ax_arrow = fig.add_subplot(gs[1, 1])\n\n    ax_style.imshow(trainer.style_image)\n    ax_style.set_title(\n        \"Style Image\", fontdict=dict(fontsize=15, color=\"b\", fontweight=\"bold\")\n    )\n\n    ax_content.imshow(trainer.content_image)\n    ax_content.set_title(\n        \"Content Image\", fontdict=dict(fontsize=15, color=\"r\", fontweight=\"bold\")\n    )\n\n    ax_output.imshow(trainer.output_image)\n    ax_output.set_title(\n        \"Output Image\", fontdict=dict(fontsize=15, color=\"m\", fontweight=\"bold\")\n    )\n\n    ax_plus.scatter(0, 0, marker=\"P\", s=1000, c=\"black\")\n    ax_arrow.arrow(0, 0, 1, 0, head_length=0.2, color=\"black\")\n\n    ax_model.set_title(\n        \"Model\",\n        fontdict=dict(fontsize=15, color=\"black\", fontweight=\"bold\"),\n        fontweight=\"bold\",\n    )\n    lx = 0.045\n    ly = 0.5\n\n    plt.sca(ax_model)\n    plt.ylim(-lx, len(all_model_layers) * lx + lx)\n\n    for i, layer in enumerate(all_model_layers):\n        if (layer in style_layers) and (layer in content_layers):\n            bbox_params = dict(\n                boxstyle=\"round\", facecolor=\"b\", edgecolor=\"r\", linewidth=5, alpha=0.5\n            )\n        elif layer in style_layers:\n            bbox_params = dict(boxstyle=\"round\", facecolor=\"b\", alpha=0.5)\n        elif layer in content_layers:\n            bbox_params = dict(boxstyle=\"round\", facecolor=\"r\", alpha=0.5)\n        else:\n            bbox_params = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5)\n\n        plt.text(\n            0.03 + i * lx,\n            ly,\n            layer,\n            rotation=90,\n            fontsize=15,\n            horizontalalignment=\"center\",\n            verticalalignment=\"center\",\n            bbox=bbox_params,\n        )\n\n    for ax in fig.get_axes()[:4]:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    for ax in fig.get_axes()[4:]:\n        plt.sca(ax)\n        plt.axis(\"off\")\n</code></pre>"},{"location":"vizualization/#src.vizualization.save_vizualizations","title":"<code>save_vizualizations(trainer, style_image_path, content_image_path, output_image_folder, trainer_vizualizations_folder)</code>","text":"<p>Saves the trainer output image in output_image_folder      and trainer vizualization in trainer_vizualizations_folder.     Both saves share the same filename. Filename is a composition of style image name     and content image name.     Saved vizualizations do not override each other, so if you save another vizualization     created with the same input images, then new vizualization get extra postfix to the filename.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>NSTImageTrainer</code> <p>Trainer to vizualize.</p> required <code>style_image_path</code> <code>str</code> <p>Path to style image.</p> required <code>content_image_path</code> <code>str</code> <p>Path to content image.</p> required <code>output_image_folder</code> <code>str</code> <p>Path to folder where output image will be stored.</p> required <code>trainer_vizualizations_folder</code> <code>str</code> <p>Path to folder where trainer vizualization will be stored.</p> required Source code in <code>src/vizualization.py</code> <pre><code>def save_vizualizations(\n    trainer, \n    style_image_path: str, \n    content_image_path: str, \n    output_image_folder: str, \n    trainer_vizualizations_folder: str):\n\"\"\"Saves the trainer output image in output_image_folder \n        and trainer vizualization in trainer_vizualizations_folder.\n        Both saves share the same filename. Filename is a composition of style image name\n        and content image name.\n        Saved vizualizations do not override each other, so if you save another vizualization\n        created with the same input images, then new vizualization get extra postfix to the filename.\n\n    Args:\n        trainer (NSTImageTrainer): Trainer to vizualize.\n        style_image_path (str): Path to style image.\n        content_image_path (str): Path to content image.\n        output_image_folder (str): Path to folder where output image will be stored.\n        trainer_vizualizations_folder (str): Path to folder where trainer vizualization will be stored.\n    \"\"\"\n    striped_style_path, _ = os.path.splitext(style_image_path)\n    striped_content_path, _ = os.path.splitext(content_image_path)\n\n    style_img_name = os.path.split(striped_style_path)[-1]\n    content_img_name = os.path.split(striped_content_path)[-1]\n    ext = \"jpg\"\n\n    img_name = f\"{style_img_name}__{content_img_name}\"\n    result_path = os.path.join(output_image_folder, f\"{img_name}.{ext}\")\n    trainer_path = os.path.join(trainer_vizualizations_folder, f\"{img_name}.{ext}\")\n\n    if os.path.exists(result_path):\n        postfix = __find_unique_postfix(result_path)\n        result_path = os.path.join(output_image_folder, f\"{img_name}{postfix}.{ext}\")\n        trainer_path = os.path.join(trainer_vizualizations_folder, f\"{img_name}{postfix}.{ext}\")\n\n    trainer.output_image.save(result_path)\n\n    plot_trainer(trainer)\n    plt.savefig(trainer_path)\n</code></pre>"}]}