# Table of contents

- [Project description:](#project-description)
- [Environment setup:](#environment-setup)
  - [Without virtual envs:](#without-virtual-envs)
  - [Conda installation:](#conda-installation)
  - [Pip installation:](#pip-installation)
- [Quick start:](#quick-start)

# Project description:

The goal of this project was to create neural style transfer (NST) model, that allows user to create unique and cool graphics without ability to draw ;\) by transfering style of one image to another.  
Details how NST work, can be found in this [paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf). This project was also inspired by this [tensorflow tutorial](https://www.tensorflow.org/tutorials/generative/style_transfer?hl=pl) and [Andrew Ng CNN course](https://www.coursera.org/learn/convolutional-neural-networks). 
<!-- This project was inspired by [tensorflow tutorial](https://www.tensorflow.org/tutorials/generative/style_transfer?hl=pl) and [Andrew Ng CNN course](https://www.coursera.org/learn/convolutional-neural-networks). Details how NST work, can be found in [arxiv paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), but briefly:  
NST is based on deep neural network (in our case it is VGG19) that is pre-trained to perform some object classification. On the begging style and content images are passed trough the backbone model. During this pass output of some of layers is captured, and then used in special loss function that combines style and content loss functions. After that instead of train model, content image is trained. Content loss function ensure that generated image will still remind similar to content image, but on  -->

![Example image generated by model](https://i.imgur.com/ho8N71h.jpeg)
*Example image generated by model*  
  

# Environment setup:

`Note:`  
*Environment setups presented below do not include GPU acceleration initialization. If you want to use GPU, you need to install proper CUDA for your graphic card. Details how to do it, can be found on [tensorflow installation guide](https://www.tensorflow.org/install/pip?hl=en).*

----

## Without virtual envs:
*This approach isn't recommended due to potential dependency conflicts that may occur during installation process, but if it will work in your default environment it would be the fastest way.*
```console
pip install -r ./requirements/raw_requirements.txt
```
## Conda installation:
```console
conda env create --file ./requirements/environment.yaml
```


## Pip installation:

----

`Note:`  
*Pip installation require python in version between 3.7-3.10*  

----

*If you want to create virtual env by your own in different way (for example using ide), then to it and go to step four.*
1. Install library for virtual environments if you don't have it yet.
```console
pip install virtualenv
```
2. Create new virtual environment.
```console
virtualenv nst_env
```
3. Activate it  

- Windows:

```console
./nst_env/Scripts/Activate.ps1
```
- Linux:

```console
source ./nst_env/bin/activate
```

4. Install requirements from file:
```console
pip install -r ./requirements/pip_requirements.txt
```

# Quick start:

If you want to use your own images you should store them in `.\images\styles` and `.\images\contents` folders, but you may use example [style](images\styles\demo_picasso_music.jpg) and [content](images\contents\demo_pablo_picasso.jpg) images prepared for you.  
In the project folder there are two notebooks that implement generation process. [`nst_training.ipynb`](nst_training.ipynb) where you do not need to care about training parameters, but more computational power is required (it is good choice on start) and [`manual_training.ipynb`](manual_training.ipynb), which is faster, but provides only one generated image at the time (you can continue with it, when you want learn something about hyperparameters used during training - there are described there in detail). Notebooks are self explanatory, so just open one of them, and follow the descriptions.



<!-- /  
├── README.md  
├── images/  
│   ├── contents/  
│   ├── results/  
│   ├── styles/  
│   └── trainers/  
├── src/  
│   ├── utils/  
│   │   ├── \_\_init\_\_.py  
│   │   ├── randomizers.py  
│   │   └── tf_utils.py  
│   ├── model.py  
│   ├── selectioner.py  
│   └── visualization.py     -->
